05-25 10:05:10 model_name: new
05-25 10:05:10 Domain: exp
05-25 10:05:10 source: CWRU_0,CWRU_2
05-25 10:05:10 target: JNU_0
05-25 10:05:10 data_dir: ../burning_event/dataset
05-25 10:05:10 train_mode: multi_source
05-25 10:05:10 cuda_device: 0
05-25 10:05:10 save_dir: ./ckpt
05-25 10:05:10 max_epoch: 50
05-25 10:05:10 batch_size: 32
05-25 10:05:10 num_workers: 0
05-25 10:05:10 signal_size: 2048
05-25 10:05:10 random_state: 927
05-25 10:05:10 project: GOOD
05-25 10:05:10 fac: 0
05-25 10:05:10 adv: 1
05-25 10:05:10 recon: 0
05-25 10:05:10 iet: 0
05-25 10:05:10 normlizetype: mean-std
05-25 10:05:10 opt: adam
05-25 10:05:10 lr: 0.001
05-25 10:05:10 momentum: 0.9
05-25 10:05:10 betas: (0.9, 0.999)
05-25 10:05:10 weight_decay: 0.001
05-25 10:05:10 lr_scheduler: stepLR
05-25 10:05:10 gamma: 0.2
05-25 10:05:10 steps: 20
05-25 10:05:10 tradeoff: ['exp', 'exp', 'exp']
05-25 10:05:10 dropout: 0.2
05-25 10:05:10 save: False
05-25 10:05:10 load_path: 
05-25 10:05:10 tsne: False
05-25 10:05:10 save_path: ./ckpt/new/multi_source/[CWRU_0CWRU_2]To[JNU_0]_0525-100510
05-25 10:05:10 Detect 3 classes: ['inner', 'normal', 'outer']
05-25 10:05:11 using 1 / 2 gpus
05-25 10:05:13 Source set CWRU_0 number of samples 177.
05-25 10:05:13 Source set CWRU_2 number of samples 177.
05-25 10:05:15 target training set number of samples 729.
05-25 10:05:15 target validation set number of samples 366.
05-25 10:05:15 -----Epoch 1/50-----
05-25 10:05:15 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:16 Train-Acc Source Data 0: 0.9474
05-25 10:05:16 Train-Acc Source Data 1: 0.9432
05-25 10:05:16 Train-Loss Source Classifier: 1.8353
05-25 10:05:16 Train-Loss adv loss: 3.2521
05-25 10:05:16 Train-Loss facto loss: 0.2136
05-25 10:05:16 Train-Loss machine domain loss: 1.2085
05-25 10:05:16 Train-Loss info preserve: 2.0000
05-25 10:05:16 Train-Loss distribution gap: 0.0004
05-25 10:05:16 Val-Acc Target Data: 0.4989
05-25 10:05:16 The best model epoch 1, val-acc 0.4989
05-25 10:05:16 -----Epoch 2/50-----
05-25 10:05:16 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:17 Train-Acc Source Data 0: 1.0000
05-25 10:05:17 Train-Acc Source Data 1: 1.0000
05-25 10:05:17 Train-Loss Source Classifier: 1.3429
05-25 10:05:17 Train-Loss adv loss: 3.0723
05-25 10:05:17 Train-Loss facto loss: 0.1916
05-25 10:05:17 Train-Loss machine domain loss: 0.9231
05-25 10:05:17 Train-Loss info preserve: 2.0000
05-25 10:05:17 Train-Loss distribution gap: 0.0004
05-25 10:05:17 Val-Acc Target Data: 0.6458
05-25 10:05:17 The best model epoch 2, val-acc 0.6458
05-25 10:05:17 -----Epoch 3/50-----
05-25 10:05:17 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:17 Train-Acc Source Data 0: 1.0000
05-25 10:05:17 Train-Acc Source Data 1: 1.0000
05-25 10:05:17 Train-Loss Source Classifier: 1.0033
05-25 10:05:17 Train-Loss adv loss: 2.9160
05-25 10:05:17 Train-Loss facto loss: 0.1853
05-25 10:05:17 Train-Loss machine domain loss: 0.7375
05-25 10:05:17 Train-Loss info preserve: 2.0000
05-25 10:05:17 Train-Loss distribution gap: 0.0003
05-25 10:05:17 Val-Acc Target Data: 0.6380
05-25 10:05:17 The best model epoch 2, val-acc 0.6458
05-25 10:05:17 -----Epoch 4/50-----
05-25 10:05:17 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:17 Train-Acc Source Data 0: 1.0000
05-25 10:05:17 Train-Acc Source Data 1: 1.0000
05-25 10:05:17 Train-Loss Source Classifier: 0.7612
05-25 10:05:17 Train-Loss adv loss: 2.8207
05-25 10:05:17 Train-Loss facto loss: 0.1859
05-25 10:05:17 Train-Loss machine domain loss: 0.6035
05-25 10:05:17 Train-Loss info preserve: 2.0000
05-25 10:05:17 Train-Loss distribution gap: 0.0002
05-25 10:05:17 Val-Acc Target Data: 0.6589
05-25 10:05:17 The best model epoch 4, val-acc 0.6589
05-25 10:05:17 -----Epoch 5/50-----
05-25 10:05:17 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:18 Train-Acc Source Data 0: 1.0000
05-25 10:05:18 Train-Acc Source Data 1: 1.0000
05-25 10:05:18 Train-Loss Source Classifier: 0.5900
05-25 10:05:18 Train-Loss adv loss: 2.7696
05-25 10:05:18 Train-Loss facto loss: 0.1880
05-25 10:05:18 Train-Loss machine domain loss: 0.5025
05-25 10:05:18 Train-Loss info preserve: 2.0000
05-25 10:05:18 Train-Loss distribution gap: 0.0002
05-25 10:05:18 Val-Acc Target Data: 0.6432
05-25 10:05:18 The best model epoch 4, val-acc 0.6589
05-25 10:05:18 -----Epoch 6/50-----
05-25 10:05:18 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:18 Train-Acc Source Data 0: 1.0000
05-25 10:05:18 Train-Acc Source Data 1: 1.0000
05-25 10:05:18 Train-Loss Source Classifier: 0.4677
05-25 10:05:18 Train-Loss adv loss: 2.7433
05-25 10:05:18 Train-Loss facto loss: 0.1892
05-25 10:05:18 Train-Loss machine domain loss: 0.4242
05-25 10:05:18 Train-Loss info preserve: 2.0000
05-25 10:05:18 Train-Loss distribution gap: 0.0002
05-25 10:05:18 Val-Acc Target Data: 0.6510
05-25 10:05:18 The best model epoch 4, val-acc 0.6589
05-25 10:05:18 -----Epoch 7/50-----
05-25 10:05:18 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:18 Train-Acc Source Data 0: 1.0000
05-25 10:05:18 Train-Acc Source Data 1: 1.0000
05-25 10:05:18 Train-Loss Source Classifier: 0.3785
05-25 10:05:18 Train-Loss adv loss: 2.7295
05-25 10:05:18 Train-Loss facto loss: 0.1898
05-25 10:05:18 Train-Loss machine domain loss: 0.3624
05-25 10:05:18 Train-Loss info preserve: 2.0000
05-25 10:05:18 Train-Loss distribution gap: 0.0002
05-25 10:05:18 Val-Acc Target Data: 0.6510
05-25 10:05:18 The best model epoch 4, val-acc 0.6589
05-25 10:05:18 -----Epoch 8/50-----
05-25 10:05:18 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:18 Train-Acc Source Data 0: 1.0000
05-25 10:05:18 Train-Acc Source Data 1: 1.0000
05-25 10:05:18 Train-Loss Source Classifier: 0.3126
05-25 10:05:18 Train-Loss adv loss: 2.7218
05-25 10:05:18 Train-Loss facto loss: 0.1900
05-25 10:05:18 Train-Loss machine domain loss: 0.3130
05-25 10:05:18 Train-Loss info preserve: 2.0000
05-25 10:05:18 Train-Loss distribution gap: 0.0002
05-25 10:05:18 Val-Acc Target Data: 0.6458
05-25 10:05:18 The best model epoch 4, val-acc 0.6589
05-25 10:05:18 -----Epoch 9/50-----
05-25 10:05:18 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:19 Train-Acc Source Data 0: 1.0000
05-25 10:05:19 Train-Acc Source Data 1: 1.0000
05-25 10:05:19 Train-Loss Source Classifier: 0.2629
05-25 10:05:19 Train-Loss adv loss: 2.7169
05-25 10:05:19 Train-Loss facto loss: 0.1903
05-25 10:05:19 Train-Loss machine domain loss: 0.2730
05-25 10:05:19 Train-Loss info preserve: 2.0000
05-25 10:05:19 Train-Loss distribution gap: 0.0002
05-25 10:05:19 Val-Acc Target Data: 0.6484
05-25 10:05:19 The best model epoch 4, val-acc 0.6589
05-25 10:05:19 -----Epoch 10/50-----
05-25 10:05:19 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:19 Train-Acc Source Data 0: 1.0000
05-25 10:05:19 Train-Acc Source Data 1: 1.0000
05-25 10:05:19 Train-Loss Source Classifier: 0.2247
05-25 10:05:19 Train-Loss adv loss: 2.7138
05-25 10:05:19 Train-Loss facto loss: 0.1900
05-25 10:05:19 Train-Loss machine domain loss: 0.2404
05-25 10:05:19 Train-Loss info preserve: 2.0000
05-25 10:05:19 Train-Loss distribution gap: 0.0003
05-25 10:05:19 Val-Acc Target Data: 0.6536
05-25 10:05:19 The best model epoch 4, val-acc 0.6589
05-25 10:05:19 -----Epoch 11/50-----
05-25 10:05:19 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:19 Train-Acc Source Data 0: 1.0000
05-25 10:05:19 Train-Acc Source Data 1: 1.0000
05-25 10:05:19 Train-Loss Source Classifier: 0.1947
05-25 10:05:19 Train-Loss adv loss: 2.7118
05-25 10:05:19 Train-Loss facto loss: 0.1898
05-25 10:05:19 Train-Loss machine domain loss: 0.2134
05-25 10:05:19 Train-Loss info preserve: 2.0000
05-25 10:05:19 Train-Loss distribution gap: 0.0002
05-25 10:05:19 Val-Acc Target Data: 0.6458
05-25 10:05:19 The best model epoch 4, val-acc 0.6589
05-25 10:05:19 -----Epoch 12/50-----
05-25 10:05:19 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:20 Train-Acc Source Data 0: 1.0000
05-25 10:05:20 Train-Acc Source Data 1: 1.0000
05-25 10:05:20 Train-Loss Source Classifier: 0.1707
05-25 10:05:20 Train-Loss adv loss: 2.7104
05-25 10:05:20 Train-Loss facto loss: 0.1889
05-25 10:05:20 Train-Loss machine domain loss: 0.1911
05-25 10:05:20 Train-Loss info preserve: 2.0000
05-25 10:05:20 Train-Loss distribution gap: 0.0002
05-25 10:05:20 Val-Acc Target Data: 0.6589
05-25 10:05:20 The best model epoch 12, val-acc 0.6589
05-25 10:05:20 -----Epoch 13/50-----
05-25 10:05:20 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:20 Train-Acc Source Data 0: 1.0000
05-25 10:05:20 Train-Acc Source Data 1: 1.0000
05-25 10:05:20 Train-Loss Source Classifier: 0.1514
05-25 10:05:20 Train-Loss adv loss: 2.7095
05-25 10:05:20 Train-Loss facto loss: 0.1895
05-25 10:05:20 Train-Loss machine domain loss: 0.1722
05-25 10:05:20 Train-Loss info preserve: 2.0000
05-25 10:05:20 Train-Loss distribution gap: 0.0002
05-25 10:05:20 Val-Acc Target Data: 0.6615
05-25 10:05:20 The best model epoch 13, val-acc 0.6615
05-25 10:05:20 -----Epoch 14/50-----
05-25 10:05:20 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:20 Train-Acc Source Data 0: 1.0000
05-25 10:05:20 Train-Acc Source Data 1: 1.0000
05-25 10:05:20 Train-Loss Source Classifier: 0.1353
05-25 10:05:20 Train-Loss adv loss: 2.7090
05-25 10:05:20 Train-Loss facto loss: 0.1880
05-25 10:05:20 Train-Loss machine domain loss: 0.1562
05-25 10:05:20 Train-Loss info preserve: 2.0000
05-25 10:05:20 Train-Loss distribution gap: 0.0001
05-25 10:05:20 Val-Acc Target Data: 0.6615
05-25 10:05:20 The best model epoch 14, val-acc 0.6615
05-25 10:05:20 -----Epoch 15/50-----
05-25 10:05:20 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:20 Train-Acc Source Data 0: 1.0000
05-25 10:05:20 Train-Acc Source Data 1: 1.0000
05-25 10:05:20 Train-Loss Source Classifier: 0.1223
05-25 10:05:20 Train-Loss adv loss: 2.7087
05-25 10:05:20 Train-Loss facto loss: 0.1871
05-25 10:05:20 Train-Loss machine domain loss: 0.1426
05-25 10:05:20 Train-Loss info preserve: 2.0000
05-25 10:05:20 Train-Loss distribution gap: 0.0002
05-25 10:05:20 Val-Acc Target Data: 0.6484
05-25 10:05:20 The best model epoch 14, val-acc 0.6615
05-25 10:05:20 -----Epoch 16/50-----
05-25 10:05:20 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:21 Train-Acc Source Data 0: 1.0000
05-25 10:05:21 Train-Acc Source Data 1: 1.0000
05-25 10:05:21 Train-Loss Source Classifier: 0.1112
05-25 10:05:21 Train-Loss adv loss: 2.7085
05-25 10:05:21 Train-Loss facto loss: 0.1867
05-25 10:05:21 Train-Loss machine domain loss: 0.1307
05-25 10:05:21 Train-Loss info preserve: 2.0000
05-25 10:05:21 Train-Loss distribution gap: 0.0002
05-25 10:05:21 Val-Acc Target Data: 0.6641
05-25 10:05:21 The best model epoch 16, val-acc 0.6641
05-25 10:05:21 -----Epoch 17/50-----
05-25 10:05:21 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:21 Train-Acc Source Data 0: 1.0000
05-25 10:05:21 Train-Acc Source Data 1: 1.0000
05-25 10:05:21 Train-Loss Source Classifier: 0.1016
05-25 10:05:21 Train-Loss adv loss: 2.7084
05-25 10:05:21 Train-Loss facto loss: 0.1857
05-25 10:05:21 Train-Loss machine domain loss: 0.1205
05-25 10:05:21 Train-Loss info preserve: 2.0000
05-25 10:05:21 Train-Loss distribution gap: 0.0002
05-25 10:05:21 Val-Acc Target Data: 0.6172
05-25 10:05:21 The best model epoch 16, val-acc 0.6641
05-25 10:05:21 -----Epoch 18/50-----
05-25 10:05:21 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:21 Train-Acc Source Data 0: 1.0000
05-25 10:05:21 Train-Acc Source Data 1: 1.0000
05-25 10:05:21 Train-Loss Source Classifier: 0.0933
05-25 10:05:21 Train-Loss adv loss: 2.7083
05-25 10:05:21 Train-Loss facto loss: 0.1851
05-25 10:05:21 Train-Loss machine domain loss: 0.1116
05-25 10:05:21 Train-Loss info preserve: 2.0000
05-25 10:05:21 Train-Loss distribution gap: 0.0002
05-25 10:05:21 Val-Acc Target Data: 0.6562
05-25 10:05:21 The best model epoch 16, val-acc 0.6641
05-25 10:05:21 -----Epoch 19/50-----
05-25 10:05:21 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:22 Train-Acc Source Data 0: 1.0000
05-25 10:05:22 Train-Acc Source Data 1: 1.0000
05-25 10:05:22 Train-Loss Source Classifier: 0.0866
05-25 10:05:22 Train-Loss adv loss: 2.7083
05-25 10:05:22 Train-Loss facto loss: 0.1841
05-25 10:05:22 Train-Loss machine domain loss: 0.1037
05-25 10:05:22 Train-Loss info preserve: 2.0000
05-25 10:05:22 Train-Loss distribution gap: 0.0002
05-25 10:05:22 Val-Acc Target Data: 0.6432
05-25 10:05:22 The best model epoch 16, val-acc 0.6641
05-25 10:05:22 -----Epoch 20/50-----
05-25 10:05:22 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 10:05:22 Train-Acc Source Data 0: 1.0000
05-25 10:05:22 Train-Acc Source Data 1: 1.0000
05-25 10:05:22 Train-Loss Source Classifier: 0.0805
05-25 10:05:22 Train-Loss adv loss: 2.7082
05-25 10:05:22 Train-Loss facto loss: 0.1836
05-25 10:05:22 Train-Loss machine domain loss: 0.0966
05-25 10:05:22 Train-Loss info preserve: 2.0000
05-25 10:05:22 Train-Loss distribution gap: 0.0002
05-25 10:05:22 Val-Acc Target Data: 0.6589
05-25 10:05:22 The best model epoch 16, val-acc 0.6641
05-25 10:05:22 -----Epoch 21/50-----
05-25 10:05:22 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:22 Train-Acc Source Data 0: 1.0000
05-25 10:05:22 Train-Acc Source Data 1: 1.0000
05-25 10:05:22 Train-Loss Source Classifier: 0.0770
05-25 10:05:22 Train-Loss adv loss: 2.7080
05-25 10:05:22 Train-Loss facto loss: 0.1832
05-25 10:05:22 Train-Loss machine domain loss: 0.0926
05-25 10:05:22 Train-Loss info preserve: 2.0000
05-25 10:05:22 Train-Loss distribution gap: 0.0002
05-25 10:05:22 Val-Acc Target Data: 0.6693
05-25 10:05:22 The best model epoch 21, val-acc 0.6693
05-25 10:05:22 -----Epoch 22/50-----
05-25 10:05:22 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:23 Train-Acc Source Data 0: 1.0000
05-25 10:05:23 Train-Acc Source Data 1: 1.0000
05-25 10:05:23 Train-Loss Source Classifier: 0.0757
05-25 10:05:23 Train-Loss adv loss: 2.7080
05-25 10:05:23 Train-Loss facto loss: 0.1816
05-25 10:05:23 Train-Loss machine domain loss: 0.0913
05-25 10:05:23 Train-Loss info preserve: 2.0000
05-25 10:05:23 Train-Loss distribution gap: 0.0001
05-25 10:05:23 Val-Acc Target Data: 0.6380
05-25 10:05:23 The best model epoch 21, val-acc 0.6693
05-25 10:05:23 -----Epoch 23/50-----
05-25 10:05:23 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:23 Train-Acc Source Data 0: 1.0000
05-25 10:05:23 Train-Acc Source Data 1: 1.0000
05-25 10:05:23 Train-Loss Source Classifier: 0.0747
05-25 10:05:23 Train-Loss adv loss: 2.7079
05-25 10:05:23 Train-Loss facto loss: 0.1811
05-25 10:05:23 Train-Loss machine domain loss: 0.0901
05-25 10:05:23 Train-Loss info preserve: 2.0000
05-25 10:05:23 Train-Loss distribution gap: 0.0002
05-25 10:05:23 Val-Acc Target Data: 0.6250
05-25 10:05:23 The best model epoch 21, val-acc 0.6693
05-25 10:05:23 -----Epoch 24/50-----
05-25 10:05:23 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:23 Train-Acc Source Data 0: 1.0000
05-25 10:05:23 Train-Acc Source Data 1: 1.0000
05-25 10:05:23 Train-Loss Source Classifier: 0.0736
05-25 10:05:23 Train-Loss adv loss: 2.7079
05-25 10:05:23 Train-Loss facto loss: 0.1807
05-25 10:05:23 Train-Loss machine domain loss: 0.0889
05-25 10:05:23 Train-Loss info preserve: 2.0000
05-25 10:05:23 Train-Loss distribution gap: 0.0002
05-25 10:05:23 Val-Acc Target Data: 0.6380
05-25 10:05:23 The best model epoch 21, val-acc 0.6693
05-25 10:05:23 -----Epoch 25/50-----
05-25 10:05:23 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:23 Train-Acc Source Data 0: 1.0000
05-25 10:05:23 Train-Acc Source Data 1: 1.0000
05-25 10:05:23 Train-Loss Source Classifier: 0.0727
05-25 10:05:23 Train-Loss adv loss: 2.7079
05-25 10:05:23 Train-Loss facto loss: 0.1806
05-25 10:05:23 Train-Loss machine domain loss: 0.0877
05-25 10:05:23 Train-Loss info preserve: 2.0000
05-25 10:05:23 Train-Loss distribution gap: 0.0002
05-25 10:05:23 Val-Acc Target Data: 0.6172
05-25 10:05:23 The best model epoch 21, val-acc 0.6693
05-25 10:05:23 -----Epoch 26/50-----
05-25 10:05:23 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:24 Train-Acc Source Data 0: 1.0000
05-25 10:05:24 Train-Acc Source Data 1: 1.0000
05-25 10:05:24 Train-Loss Source Classifier: 0.0716
05-25 10:05:24 Train-Loss adv loss: 2.7079
05-25 10:05:24 Train-Loss facto loss: 0.1803
05-25 10:05:24 Train-Loss machine domain loss: 0.0866
05-25 10:05:24 Train-Loss info preserve: 2.0000
05-25 10:05:24 Train-Loss distribution gap: 0.0001
05-25 10:05:24 Val-Acc Target Data: 0.6224
05-25 10:05:24 The best model epoch 21, val-acc 0.6693
05-25 10:05:24 -----Epoch 27/50-----
05-25 10:05:24 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:24 Train-Acc Source Data 0: 1.0000
05-25 10:05:24 Train-Acc Source Data 1: 1.0000
05-25 10:05:24 Train-Loss Source Classifier: 0.0706
05-25 10:05:24 Train-Loss adv loss: 2.7079
05-25 10:05:24 Train-Loss facto loss: 0.1798
05-25 10:05:24 Train-Loss machine domain loss: 0.0854
05-25 10:05:24 Train-Loss info preserve: 2.0000
05-25 10:05:24 Train-Loss distribution gap: 0.0002
05-25 10:05:24 Val-Acc Target Data: 0.6224
05-25 10:05:24 The best model epoch 21, val-acc 0.6693
05-25 10:05:24 -----Epoch 28/50-----
05-25 10:05:24 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:24 Train-Acc Source Data 0: 1.0000
05-25 10:05:24 Train-Acc Source Data 1: 1.0000
05-25 10:05:24 Train-Loss Source Classifier: 0.0697
05-25 10:05:24 Train-Loss adv loss: 2.7078
05-25 10:05:24 Train-Loss facto loss: 0.1792
05-25 10:05:24 Train-Loss machine domain loss: 0.0842
05-25 10:05:24 Train-Loss info preserve: 2.0000
05-25 10:05:24 Train-Loss distribution gap: 0.0001
05-25 10:05:24 Val-Acc Target Data: 0.6406
05-25 10:05:24 The best model epoch 21, val-acc 0.6693
05-25 10:05:24 -----Epoch 29/50-----
05-25 10:05:24 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:25 Train-Acc Source Data 0: 1.0000
05-25 10:05:25 Train-Acc Source Data 1: 1.0000
05-25 10:05:25 Train-Loss Source Classifier: 0.0687
05-25 10:05:25 Train-Loss adv loss: 2.7079
05-25 10:05:25 Train-Loss facto loss: 0.1791
05-25 10:05:25 Train-Loss machine domain loss: 0.0831
05-25 10:05:25 Train-Loss info preserve: 2.0000
05-25 10:05:25 Train-Loss distribution gap: 0.0001
05-25 10:05:25 Val-Acc Target Data: 0.6354
05-25 10:05:25 The best model epoch 21, val-acc 0.6693
05-25 10:05:25 -----Epoch 30/50-----
05-25 10:05:25 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:25 Train-Acc Source Data 0: 1.0000
05-25 10:05:25 Train-Acc Source Data 1: 1.0000
05-25 10:05:25 Train-Loss Source Classifier: 0.0677
05-25 10:05:25 Train-Loss adv loss: 2.7079
05-25 10:05:25 Train-Loss facto loss: 0.1793
05-25 10:05:25 Train-Loss machine domain loss: 0.0820
05-25 10:05:25 Train-Loss info preserve: 2.0000
05-25 10:05:25 Train-Loss distribution gap: 0.0001
05-25 10:05:25 Val-Acc Target Data: 0.6406
05-25 10:05:25 The best model epoch 21, val-acc 0.6693
05-25 10:05:25 -----Epoch 31/50-----
05-25 10:05:25 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:25 Train-Acc Source Data 0: 1.0000
05-25 10:05:25 Train-Acc Source Data 1: 1.0000
05-25 10:05:25 Train-Loss Source Classifier: 0.0668
05-25 10:05:25 Train-Loss adv loss: 2.7079
05-25 10:05:25 Train-Loss facto loss: 0.1786
05-25 10:05:25 Train-Loss machine domain loss: 0.0809
05-25 10:05:25 Train-Loss info preserve: 2.0000
05-25 10:05:25 Train-Loss distribution gap: 0.0002
05-25 10:05:25 Val-Acc Target Data: 0.6432
05-25 10:05:25 The best model epoch 21, val-acc 0.6693
05-25 10:05:25 -----Epoch 32/50-----
05-25 10:05:25 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:26 Train-Acc Source Data 0: 1.0000
05-25 10:05:26 Train-Acc Source Data 1: 1.0000
05-25 10:05:26 Train-Loss Source Classifier: 0.0659
05-25 10:05:26 Train-Loss adv loss: 2.7079
05-25 10:05:26 Train-Loss facto loss: 0.1783
05-25 10:05:26 Train-Loss machine domain loss: 0.0798
05-25 10:05:26 Train-Loss info preserve: 2.0000
05-25 10:05:26 Train-Loss distribution gap: 0.0002
05-25 10:05:26 Val-Acc Target Data: 0.6276
05-25 10:05:26 The best model epoch 21, val-acc 0.6693
05-25 10:05:26 -----Epoch 33/50-----
05-25 10:05:26 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:26 Train-Acc Source Data 0: 1.0000
05-25 10:05:26 Train-Acc Source Data 1: 1.0000
05-25 10:05:26 Train-Loss Source Classifier: 0.0650
05-25 10:05:26 Train-Loss adv loss: 2.7079
05-25 10:05:26 Train-Loss facto loss: 0.1784
05-25 10:05:26 Train-Loss machine domain loss: 0.0786
05-25 10:05:26 Train-Loss info preserve: 2.0000
05-25 10:05:26 Train-Loss distribution gap: 0.0001
05-25 10:05:26 Val-Acc Target Data: 0.6380
05-25 10:05:26 The best model epoch 21, val-acc 0.6693
05-25 10:05:26 -----Epoch 34/50-----
05-25 10:05:26 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:26 Train-Acc Source Data 0: 1.0000
05-25 10:05:26 Train-Acc Source Data 1: 1.0000
05-25 10:05:26 Train-Loss Source Classifier: 0.0641
05-25 10:05:26 Train-Loss adv loss: 2.7079
05-25 10:05:26 Train-Loss facto loss: 0.1777
05-25 10:05:26 Train-Loss machine domain loss: 0.0776
05-25 10:05:26 Train-Loss info preserve: 2.0000
05-25 10:05:26 Train-Loss distribution gap: 0.0002
05-25 10:05:26 Val-Acc Target Data: 0.6354
05-25 10:05:26 The best model epoch 21, val-acc 0.6693
05-25 10:05:26 -----Epoch 35/50-----
05-25 10:05:26 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:26 Train-Acc Source Data 0: 1.0000
05-25 10:05:26 Train-Acc Source Data 1: 1.0000
05-25 10:05:26 Train-Loss Source Classifier: 0.0632
05-25 10:05:26 Train-Loss adv loss: 2.7079
05-25 10:05:26 Train-Loss facto loss: 0.1773
05-25 10:05:26 Train-Loss machine domain loss: 0.0765
05-25 10:05:26 Train-Loss info preserve: 2.0000
05-25 10:05:26 Train-Loss distribution gap: 0.0002
05-25 10:05:26 Val-Acc Target Data: 0.6484
05-25 10:05:26 The best model epoch 21, val-acc 0.6693
05-25 10:05:26 -----Epoch 36/50-----
05-25 10:05:26 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:27 Train-Acc Source Data 0: 1.0000
05-25 10:05:27 Train-Acc Source Data 1: 1.0000
05-25 10:05:27 Train-Loss Source Classifier: 0.0624
05-25 10:05:27 Train-Loss adv loss: 2.7078
05-25 10:05:27 Train-Loss facto loss: 0.1769
05-25 10:05:27 Train-Loss machine domain loss: 0.0754
05-25 10:05:27 Train-Loss info preserve: 2.0000
05-25 10:05:27 Train-Loss distribution gap: 0.0001
05-25 10:05:27 Val-Acc Target Data: 0.6484
05-25 10:05:27 The best model epoch 21, val-acc 0.6693
05-25 10:05:27 -----Epoch 37/50-----
05-25 10:05:27 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:27 Train-Acc Source Data 0: 1.0000
05-25 10:05:27 Train-Acc Source Data 1: 1.0000
05-25 10:05:27 Train-Loss Source Classifier: 0.0615
05-25 10:05:27 Train-Loss adv loss: 2.7079
05-25 10:05:27 Train-Loss facto loss: 0.1773
05-25 10:05:27 Train-Loss machine domain loss: 0.0744
05-25 10:05:27 Train-Loss info preserve: 2.0000
05-25 10:05:27 Train-Loss distribution gap: 0.0002
05-25 10:05:27 Val-Acc Target Data: 0.6615
05-25 10:05:27 The best model epoch 21, val-acc 0.6693
05-25 10:05:27 -----Epoch 38/50-----
05-25 10:05:27 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:27 Train-Acc Source Data 0: 1.0000
05-25 10:05:27 Train-Acc Source Data 1: 1.0000
05-25 10:05:27 Train-Loss Source Classifier: 0.0607
05-25 10:05:27 Train-Loss adv loss: 2.7079
05-25 10:05:27 Train-Loss facto loss: 0.1768
05-25 10:05:27 Train-Loss machine domain loss: 0.0734
05-25 10:05:27 Train-Loss info preserve: 2.0000
05-25 10:05:27 Train-Loss distribution gap: 0.0002
05-25 10:05:27 Val-Acc Target Data: 0.6615
05-25 10:05:27 The best model epoch 21, val-acc 0.6693
05-25 10:05:27 -----Epoch 39/50-----
05-25 10:05:27 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:28 Train-Acc Source Data 0: 1.0000
05-25 10:05:28 Train-Acc Source Data 1: 1.0000
05-25 10:05:28 Train-Loss Source Classifier: 0.0599
05-25 10:05:28 Train-Loss adv loss: 2.7079
05-25 10:05:28 Train-Loss facto loss: 0.1766
05-25 10:05:28 Train-Loss machine domain loss: 0.0724
05-25 10:05:28 Train-Loss info preserve: 2.0000
05-25 10:05:28 Train-Loss distribution gap: 0.0002
05-25 10:05:28 Val-Acc Target Data: 0.6484
05-25 10:05:28 The best model epoch 21, val-acc 0.6693
05-25 10:05:28 -----Epoch 40/50-----
05-25 10:05:28 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 10:05:28 Train-Acc Source Data 0: 1.0000
05-25 10:05:28 Train-Acc Source Data 1: 1.0000
05-25 10:05:28 Train-Loss Source Classifier: 0.0589
05-25 10:05:28 Train-Loss adv loss: 2.7079
05-25 10:05:28 Train-Loss facto loss: 0.1761
05-25 10:05:28 Train-Loss machine domain loss: 0.0714
05-25 10:05:28 Train-Loss info preserve: 2.0000
05-25 10:05:28 Train-Loss distribution gap: 0.0002
05-25 10:05:28 Val-Acc Target Data: 0.6432
05-25 10:05:28 The best model epoch 21, val-acc 0.6693
05-25 10:05:28 -----Epoch 41/50-----
05-25 10:05:28 current lr: [4e-05, 4e-05, 4e-05, 4e-05, 4e-05, 4e-05]
05-25 10:05:28 Train-Acc Source Data 0: 1.0000
05-25 10:05:28 Train-Acc Source Data 1: 1.0000
05-25 10:05:28 Train-Loss Source Classifier: 0.0585
05-25 10:05:28 Train-Loss adv loss: 2.7078
05-25 10:05:28 Train-Loss facto loss: 0.1750
05-25 10:05:28 Train-Loss machine domain loss: 0.0708
05-25 10:05:28 Train-Loss info preserve: 2.0000
05-25 10:05:28 Train-Loss distribution gap: 0.0002
05-25 10:05:28 Val-Acc Target Data: 0.6380
05-25 10:05:28 The best model epoch 21, val-acc 0.6693
05-25 10:05:28 -----Epoch 42/50-----
05-25 10:05:28 current lr: [4e-05, 4e-05, 4e-05, 4e-05, 4e-05, 4e-05]
05-25 10:05:28 Train-Acc Source Data 0: 1.0000
05-25 10:05:28 Train-Acc Source Data 1: 1.0000
05-25 10:05:28 Train-Loss Source Classifier: 0.0583
05-25 10:05:28 Train-Loss adv loss: 2.7077
05-25 10:05:28 Train-Loss facto loss: 0.1750
05-25 10:05:28 Train-Loss machine domain loss: 0.0705
05-25 10:05:28 Train-Loss info preserve: 2.0000
05-25 10:05:28 Train-Loss distribution gap: 0.0002
05-25 10:05:29 Val-Acc Target Data: 0.6432
05-25 10:05:29 The best model epoch 21, val-acc 0.6693
05-25 10:05:29 -----Epoch 43/50-----
05-25 10:05:29 current lr: [4e-05, 4e-05, 4e-05, 4e-05, 4e-05, 4e-05]
05-25 10:05:29 Train-Acc Source Data 0: 1.0000
05-25 10:05:29 Train-Acc Source Data 1: 1.0000
05-25 10:05:29 Train-Loss Source Classifier: 0.0582
05-25 10:05:29 Train-Loss adv loss: 2.7077
05-25 10:05:29 Train-Loss facto loss: 0.1747
05-25 10:05:29 Train-Loss machine domain loss: 0.0704
05-25 10:05:29 Train-Loss info preserve: 2.0000
05-25 10:05:29 Train-Loss distribution gap: 0.0002
05-25 10:05:29 Val-Acc Target Data: 0.6536
05-25 10:05:29 The best model epoch 21, val-acc 0.6693
05-25 10:05:29 -----Epoch 44/50-----
05-25 10:05:29 current lr: [4e-05, 4e-05, 4e-05, 4e-05, 4e-05, 4e-05]
05-25 10:05:29 Train-Acc Source Data 0: 1.0000
05-25 10:05:29 Train-Acc Source Data 1: 1.0000
05-25 10:05:29 Train-Loss Source Classifier: 0.0580
05-25 10:05:29 Train-Loss adv loss: 2.7076
05-25 10:05:29 Train-Loss facto loss: 0.1742
05-25 10:05:29 Train-Loss machine domain loss: 0.0701
05-25 10:05:29 Train-Loss info preserve: 2.0000
05-25 10:05:29 Train-Loss distribution gap: 0.0002
05-25 10:05:29 Val-Acc Target Data: 0.6406
05-25 10:05:29 The best model epoch 21, val-acc 0.6693
05-25 10:05:29 -----Epoch 45/50-----
05-25 10:05:29 current lr: [4e-05, 4e-05, 4e-05, 4e-05, 4e-05, 4e-05]
05-25 10:05:29 Train-Acc Source Data 0: 1.0000
05-25 10:05:29 Train-Acc Source Data 1: 1.0000
05-25 10:05:29 Train-Loss Source Classifier: 0.0578
05-25 10:05:29 Train-Loss adv loss: 2.7076
05-25 10:05:29 Train-Loss facto loss: 0.1744
05-25 10:05:29 Train-Loss machine domain loss: 0.0699
05-25 10:05:29 Train-Loss info preserve: 2.0000
05-25 10:05:29 Train-Loss distribution gap: 0.0001
05-25 10:05:29 Val-Acc Target Data: 0.6510
05-25 10:05:29 The best model epoch 21, val-acc 0.6693
05-25 10:05:29 -----Epoch 46/50-----
05-25 10:05:29 current lr: [4e-05, 4e-05, 4e-05, 4e-05, 4e-05, 4e-05]
05-25 10:05:30 Train-Acc Source Data 0: 1.0000
05-25 10:05:30 Train-Acc Source Data 1: 1.0000
05-25 10:05:30 Train-Loss Source Classifier: 0.0576
05-25 10:05:30 Train-Loss adv loss: 2.7076
05-25 10:05:30 Train-Loss facto loss: 0.1740
05-25 10:05:30 Train-Loss machine domain loss: 0.0697
05-25 10:05:30 Train-Loss info preserve: 2.0000
05-25 10:05:30 Train-Loss distribution gap: 0.0001
05-25 10:05:30 Val-Acc Target Data: 0.6406
05-25 10:05:30 The best model epoch 21, val-acc 0.6693
05-25 10:05:30 -----Epoch 47/50-----
05-25 10:05:30 current lr: [4e-05, 4e-05, 4e-05, 4e-05, 4e-05, 4e-05]
05-25 10:05:30 Train-Acc Source Data 0: 1.0000
05-25 10:05:30 Train-Acc Source Data 1: 1.0000
05-25 10:05:30 Train-Loss Source Classifier: 0.0575
05-25 10:05:30 Train-Loss adv loss: 2.7076
05-25 10:05:30 Train-Loss facto loss: 0.1741
05-25 10:05:30 Train-Loss machine domain loss: 0.0695
05-25 10:05:30 Train-Loss info preserve: 2.0000
05-25 10:05:30 Train-Loss distribution gap: 0.0001
