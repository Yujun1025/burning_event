05-25 09:28:36 model_name: new
05-25 09:28:36 Domain: exp
05-25 09:28:36 source: CWRU_0,CWRU_2
05-25 09:28:36 target: JNU_2
05-25 09:28:36 data_dir: ../burning_event/dataset
05-25 09:28:36 train_mode: multi_source
05-25 09:28:36 cuda_device: 0
05-25 09:28:36 save_dir: ./ckpt
05-25 09:28:36 max_epoch: 50
05-25 09:28:36 batch_size: 32
05-25 09:28:36 num_workers: 0
05-25 09:28:36 signal_size: 2048
05-25 09:28:36 random_state: 1024
05-25 09:28:36 project: SWEEEEP
05-25 09:28:36 fac: 0
05-25 09:28:36 adv: 1
05-25 09:28:36 recon: 0
05-25 09:28:36 iet: 0
05-25 09:28:36 normlizetype: mean-std
05-25 09:28:36 opt: adam
05-25 09:28:36 lr: 0.001
05-25 09:28:36 momentum: 0.9
05-25 09:28:36 betas: (0.9, 0.999)
05-25 09:28:36 weight_decay: 0.001
05-25 09:28:36 lr_scheduler: stepLR
05-25 09:28:36 gamma: 0.2
05-25 09:28:36 steps: 20
05-25 09:28:36 tradeoff: ['exp', 'exp', 'exp']
05-25 09:28:36 dropout: 0.2
05-25 09:28:36 save: False
05-25 09:28:36 load_path: 
05-25 09:28:36 tsne: False
05-25 09:28:36 save_path: ./ckpt/new/multi_source/[CWRU_0CWRU_2]To[JNU_2]_0525-092836
05-25 09:28:36 Detect 3 classes: ['inner', 'normal', 'outer']
05-25 09:28:37 using 1 / 2 gpus
05-25 09:28:38 Source set CWRU_0 number of samples 177.
05-25 09:28:38 Source set CWRU_2 number of samples 177.
05-25 09:28:40 target training set number of samples 729.
05-25 09:28:40 target validation set number of samples 366.
05-25 09:28:40 -----Epoch 1/50-----
05-25 09:28:40 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:42 Train-Acc Source Data 0: 0.8494
05-25 09:28:42 Train-Acc Source Data 1: 0.8750
05-25 09:28:42 Train-Loss Source Classifier: 1.9003
05-25 09:28:42 Train-Loss adv loss: 3.2514
05-25 09:28:42 Train-Loss facto loss: 0.2247
05-25 09:28:42 Train-Loss machine domain loss: 1.1836
05-25 09:28:42 Train-Loss info preserve: 2.0000
05-25 09:28:42 Train-Loss distribution gap: 0.0002
05-25 09:28:42 Val-Acc Target Data: 0.5551
05-25 09:28:42 The best model epoch 1, val-acc 0.5551
05-25 09:28:42 -----Epoch 2/50-----
05-25 09:28:42 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:42 Train-Acc Source Data 0: 1.0000
05-25 09:28:42 Train-Acc Source Data 1: 1.0000
05-25 09:28:42 Train-Loss Source Classifier: 1.3986
05-25 09:28:42 Train-Loss adv loss: 3.0701
05-25 09:28:42 Train-Loss facto loss: 0.2106
05-25 09:28:42 Train-Loss machine domain loss: 0.9016
05-25 09:28:42 Train-Loss info preserve: 2.0000
05-25 09:28:42 Train-Loss distribution gap: 0.0001
05-25 09:28:42 Val-Acc Target Data: 0.5234
05-25 09:28:42 The best model epoch 1, val-acc 0.5551
05-25 09:28:42 -----Epoch 3/50-----
05-25 09:28:42 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:43 Train-Acc Source Data 0: 1.0000
05-25 09:28:43 Train-Acc Source Data 1: 1.0000
05-25 09:28:43 Train-Loss Source Classifier: 1.0305
05-25 09:28:43 Train-Loss adv loss: 2.9120
05-25 09:28:43 Train-Loss facto loss: 0.2005
05-25 09:28:43 Train-Loss machine domain loss: 0.7205
05-25 09:28:43 Train-Loss info preserve: 2.0000
05-25 09:28:43 Train-Loss distribution gap: 0.0002
05-25 09:28:43 Val-Acc Target Data: 0.4836
05-25 09:28:43 The best model epoch 1, val-acc 0.5551
05-25 09:28:43 -----Epoch 4/50-----
05-25 09:28:43 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:43 Train-Acc Source Data 0: 1.0000
05-25 09:28:43 Train-Acc Source Data 1: 1.0000
05-25 09:28:43 Train-Loss Source Classifier: 0.7722
05-25 09:28:43 Train-Loss adv loss: 2.8168
05-25 09:28:43 Train-Loss facto loss: 0.1963
05-25 09:28:43 Train-Loss machine domain loss: 0.5911
05-25 09:28:43 Train-Loss info preserve: 2.0000
05-25 09:28:43 Train-Loss distribution gap: 0.0001
05-25 09:28:43 Val-Acc Target Data: 0.5417
05-25 09:28:43 The best model epoch 1, val-acc 0.5551
05-25 09:28:43 -----Epoch 5/50-----
05-25 09:28:43 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:43 Train-Acc Source Data 0: 1.0000
05-25 09:28:43 Train-Acc Source Data 1: 1.0000
05-25 09:28:43 Train-Loss Source Classifier: 0.5929
05-25 09:28:43 Train-Loss adv loss: 2.7669
05-25 09:28:43 Train-Loss facto loss: 0.1955
05-25 09:28:43 Train-Loss machine domain loss: 0.4949
05-25 09:28:43 Train-Loss info preserve: 2.0000
05-25 09:28:43 Train-Loss distribution gap: 0.0001
05-25 09:28:43 Val-Acc Target Data: 0.4688
05-25 09:28:43 The best model epoch 1, val-acc 0.5551
05-25 09:28:43 -----Epoch 6/50-----
05-25 09:28:43 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:43 Train-Acc Source Data 0: 1.0000
05-25 09:28:43 Train-Acc Source Data 1: 1.0000
05-25 09:28:43 Train-Loss Source Classifier: 0.4690
05-25 09:28:43 Train-Loss adv loss: 2.7427
05-25 09:28:43 Train-Loss facto loss: 0.1963
05-25 09:28:43 Train-Loss machine domain loss: 0.4205
05-25 09:28:43 Train-Loss info preserve: 2.0000
05-25 09:28:43 Train-Loss distribution gap: 0.0002
05-25 09:28:43 Val-Acc Target Data: 0.4427
05-25 09:28:43 The best model epoch 1, val-acc 0.5551
05-25 09:28:43 -----Epoch 7/50-----
05-25 09:28:43 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:44 Train-Acc Source Data 0: 1.0000
05-25 09:28:44 Train-Acc Source Data 1: 1.0000
05-25 09:28:44 Train-Loss Source Classifier: 0.3780
05-25 09:28:44 Train-Loss adv loss: 2.7301
05-25 09:28:44 Train-Loss facto loss: 0.1958
05-25 09:28:44 Train-Loss machine domain loss: 0.3616
05-25 09:28:44 Train-Loss info preserve: 2.0000
05-25 09:28:44 Train-Loss distribution gap: 0.0001
05-25 09:28:44 Val-Acc Target Data: 0.4055
05-25 09:28:44 The best model epoch 1, val-acc 0.5551
05-25 09:28:44 -----Epoch 8/50-----
05-25 09:28:44 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:44 Train-Acc Source Data 0: 1.0000
05-25 09:28:44 Train-Acc Source Data 1: 1.0000
05-25 09:28:44 Train-Loss Source Classifier: 0.3123
05-25 09:28:44 Train-Loss adv loss: 2.7228
05-25 09:28:44 Train-Loss facto loss: 0.1954
05-25 09:28:44 Train-Loss machine domain loss: 0.3142
05-25 09:28:44 Train-Loss info preserve: 2.0000
05-25 09:28:44 Train-Loss distribution gap: 0.0001
05-25 09:28:44 Val-Acc Target Data: 0.4531
05-25 09:28:44 The best model epoch 1, val-acc 0.5551
05-25 09:28:44 -----Epoch 9/50-----
05-25 09:28:44 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:44 Train-Acc Source Data 0: 1.0000
05-25 09:28:44 Train-Acc Source Data 1: 1.0000
05-25 09:28:44 Train-Loss Source Classifier: 0.2632
05-25 09:28:44 Train-Loss adv loss: 2.7180
05-25 09:28:44 Train-Loss facto loss: 0.1956
05-25 09:28:44 Train-Loss machine domain loss: 0.2757
05-25 09:28:44 Train-Loss info preserve: 2.0000
05-25 09:28:44 Train-Loss distribution gap: 0.0001
05-25 09:28:44 Val-Acc Target Data: 0.4237
05-25 09:28:44 The best model epoch 1, val-acc 0.5551
05-25 09:28:44 -----Epoch 10/50-----
05-25 09:28:44 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:44 Train-Acc Source Data 0: 1.0000
05-25 09:28:44 Train-Acc Source Data 1: 1.0000
05-25 09:28:44 Train-Loss Source Classifier: 0.2252
05-25 09:28:44 Train-Loss adv loss: 2.7148
05-25 09:28:44 Train-Loss facto loss: 0.1954
05-25 09:28:44 Train-Loss machine domain loss: 0.2439
05-25 09:28:44 Train-Loss info preserve: 2.0000
05-25 09:28:44 Train-Loss distribution gap: 0.0001
05-25 09:28:45 Val-Acc Target Data: 0.4349
05-25 09:28:45 The best model epoch 1, val-acc 0.5551
05-25 09:28:45 -----Epoch 11/50-----
05-25 09:28:45 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:45 Train-Acc Source Data 0: 1.0000
05-25 09:28:45 Train-Acc Source Data 1: 1.0000
05-25 09:28:45 Train-Loss Source Classifier: 0.1963
05-25 09:28:45 Train-Loss adv loss: 2.7127
05-25 09:28:45 Train-Loss facto loss: 0.1950
05-25 09:28:45 Train-Loss machine domain loss: 0.2174
05-25 09:28:45 Train-Loss info preserve: 2.0000
05-25 09:28:45 Train-Loss distribution gap: 0.0001
05-25 09:28:45 Val-Acc Target Data: 0.4081
05-25 09:28:45 The best model epoch 1, val-acc 0.5551
05-25 09:28:45 -----Epoch 12/50-----
05-25 09:28:45 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:45 Train-Acc Source Data 0: 1.0000
05-25 09:28:45 Train-Acc Source Data 1: 1.0000
05-25 09:28:45 Train-Loss Source Classifier: 0.1723
05-25 09:28:45 Train-Loss adv loss: 2.7113
05-25 09:28:45 Train-Loss facto loss: 0.1954
05-25 09:28:45 Train-Loss machine domain loss: 0.1952
05-25 09:28:45 Train-Loss info preserve: 2.0000
05-25 09:28:45 Train-Loss distribution gap: 0.0001
05-25 09:28:45 Val-Acc Target Data: 0.4505
05-25 09:28:45 The best model epoch 1, val-acc 0.5551
05-25 09:28:45 -----Epoch 13/50-----
05-25 09:28:45 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:45 Train-Acc Source Data 0: 1.0000
05-25 09:28:45 Train-Acc Source Data 1: 1.0000
05-25 09:28:45 Train-Loss Source Classifier: 0.1535
05-25 09:28:45 Train-Loss adv loss: 2.7103
05-25 09:28:45 Train-Loss facto loss: 0.1951
05-25 09:28:45 Train-Loss machine domain loss: 0.1764
05-25 09:28:45 Train-Loss info preserve: 2.0000
05-25 09:28:45 Train-Loss distribution gap: 0.0001
05-25 09:28:45 Val-Acc Target Data: 0.4427
05-25 09:28:45 The best model epoch 1, val-acc 0.5551
05-25 09:28:45 -----Epoch 14/50-----
05-25 09:28:45 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:46 Train-Acc Source Data 0: 1.0000
05-25 09:28:46 Train-Acc Source Data 1: 1.0000
05-25 09:28:46 Train-Loss Source Classifier: 0.1370
05-25 09:28:46 Train-Loss adv loss: 2.7096
05-25 09:28:46 Train-Loss facto loss: 0.1949
05-25 09:28:46 Train-Loss machine domain loss: 0.1603
05-25 09:28:46 Train-Loss info preserve: 2.0000
05-25 09:28:46 Train-Loss distribution gap: 0.0001
05-25 09:28:46 Val-Acc Target Data: 0.4115
05-25 09:28:46 The best model epoch 1, val-acc 0.5551
05-25 09:28:46 -----Epoch 15/50-----
05-25 09:28:46 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:46 Train-Acc Source Data 0: 1.0000
05-25 09:28:46 Train-Acc Source Data 1: 1.0000
05-25 09:28:46 Train-Loss Source Classifier: 0.1238
05-25 09:28:46 Train-Loss adv loss: 2.7092
05-25 09:28:46 Train-Loss facto loss: 0.1947
05-25 09:28:46 Train-Loss machine domain loss: 0.1464
05-25 09:28:46 Train-Loss info preserve: 2.0000
05-25 09:28:46 Train-Loss distribution gap: 0.0001
05-25 09:28:46 Val-Acc Target Data: 0.4401
05-25 09:28:46 The best model epoch 1, val-acc 0.5551
05-25 09:28:46 -----Epoch 16/50-----
05-25 09:28:46 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:46 Train-Acc Source Data 0: 1.0000
05-25 09:28:46 Train-Acc Source Data 1: 1.0000
05-25 09:28:46 Train-Loss Source Classifier: 0.1126
05-25 09:28:46 Train-Loss adv loss: 2.7089
05-25 09:28:46 Train-Loss facto loss: 0.1949
05-25 09:28:46 Train-Loss machine domain loss: 0.1345
05-25 09:28:46 Train-Loss info preserve: 2.0000
05-25 09:28:46 Train-Loss distribution gap: 0.0001
05-25 09:28:46 Val-Acc Target Data: 0.3984
05-25 09:28:46 The best model epoch 1, val-acc 0.5551
05-25 09:28:46 -----Epoch 17/50-----
05-25 09:28:46 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:47 Train-Acc Source Data 0: 1.0000
05-25 09:28:47 Train-Acc Source Data 1: 1.0000
05-25 09:28:47 Train-Loss Source Classifier: 0.1032
05-25 09:28:47 Train-Loss adv loss: 2.7088
05-25 09:28:47 Train-Loss facto loss: 0.1944
05-25 09:28:47 Train-Loss machine domain loss: 0.1240
05-25 09:28:47 Train-Loss info preserve: 2.0000
05-25 09:28:47 Train-Loss distribution gap: 0.0001
05-25 09:28:47 Val-Acc Target Data: 0.4583
05-25 09:28:47 The best model epoch 1, val-acc 0.5551
05-25 09:28:47 -----Epoch 18/50-----
05-25 09:28:47 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:47 Train-Acc Source Data 0: 1.0000
05-25 09:28:47 Train-Acc Source Data 1: 1.0000
05-25 09:28:47 Train-Loss Source Classifier: 0.0954
05-25 09:28:47 Train-Loss adv loss: 2.7086
05-25 09:28:47 Train-Loss facto loss: 0.1941
05-25 09:28:47 Train-Loss machine domain loss: 0.1149
05-25 09:28:47 Train-Loss info preserve: 2.0000
05-25 09:28:47 Train-Loss distribution gap: 0.0001
05-25 09:28:47 Val-Acc Target Data: 0.4635
05-25 09:28:47 The best model epoch 1, val-acc 0.5551
05-25 09:28:47 -----Epoch 19/50-----
05-25 09:28:47 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:47 Train-Acc Source Data 0: 1.0000
05-25 09:28:47 Train-Acc Source Data 1: 1.0000
05-25 09:28:47 Train-Loss Source Classifier: 0.0884
05-25 09:28:47 Train-Loss adv loss: 2.7086
05-25 09:28:47 Train-Loss facto loss: 0.1947
05-25 09:28:47 Train-Loss machine domain loss: 0.1068
05-25 09:28:47 Train-Loss info preserve: 2.0000
05-25 09:28:47 Train-Loss distribution gap: 0.0002
05-25 09:28:47 Val-Acc Target Data: 0.3873
05-25 09:28:47 The best model epoch 1, val-acc 0.5551
05-25 09:28:47 -----Epoch 20/50-----
05-25 09:28:47 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
05-25 09:28:47 Train-Acc Source Data 0: 1.0000
05-25 09:28:47 Train-Acc Source Data 1: 1.0000
05-25 09:28:47 Train-Loss Source Classifier: 0.0831
05-25 09:28:47 Train-Loss adv loss: 2.7090
05-25 09:28:47 Train-Loss facto loss: 0.1940
05-25 09:28:47 Train-Loss machine domain loss: 0.1000
05-25 09:28:47 Train-Loss info preserve: 2.0000
05-25 09:28:47 Train-Loss distribution gap: 0.0002
05-25 09:28:48 Val-Acc Target Data: 0.4401
05-25 09:28:48 The best model epoch 1, val-acc 0.5551
05-25 09:28:48 -----Epoch 21/50-----
05-25 09:28:48 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:48 Train-Acc Source Data 0: 1.0000
05-25 09:28:48 Train-Acc Source Data 1: 1.0000
05-25 09:28:48 Train-Loss Source Classifier: 0.0786
05-25 09:28:48 Train-Loss adv loss: 2.7085
05-25 09:28:48 Train-Loss facto loss: 0.1942
05-25 09:28:48 Train-Loss machine domain loss: 0.0957
05-25 09:28:48 Train-Loss info preserve: 2.0000
05-25 09:28:48 Train-Loss distribution gap: 0.0001
05-25 09:28:48 Val-Acc Target Data: 0.4505
05-25 09:28:48 The best model epoch 1, val-acc 0.5551
05-25 09:28:48 -----Epoch 22/50-----
05-25 09:28:48 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:48 Train-Acc Source Data 0: 1.0000
05-25 09:28:48 Train-Acc Source Data 1: 1.0000
05-25 09:28:48 Train-Loss Source Classifier: 0.0769
05-25 09:28:48 Train-Loss adv loss: 2.7084
05-25 09:28:48 Train-Loss facto loss: 0.1932
05-25 09:28:48 Train-Loss machine domain loss: 0.0943
05-25 09:28:48 Train-Loss info preserve: 2.0000
05-25 09:28:48 Train-Loss distribution gap: 0.0001
05-25 09:28:48 Val-Acc Target Data: 0.4479
05-25 09:28:48 The best model epoch 1, val-acc 0.5551
05-25 09:28:48 -----Epoch 23/50-----
05-25 09:28:48 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:48 Train-Acc Source Data 0: 1.0000
05-25 09:28:48 Train-Acc Source Data 1: 1.0000
05-25 09:28:48 Train-Loss Source Classifier: 0.0758
05-25 09:28:48 Train-Loss adv loss: 2.7084
05-25 09:28:48 Train-Loss facto loss: 0.1930
05-25 09:28:48 Train-Loss machine domain loss: 0.0930
05-25 09:28:48 Train-Loss info preserve: 2.0000
05-25 09:28:48 Train-Loss distribution gap: 0.0001
05-25 09:28:48 Val-Acc Target Data: 0.4557
05-25 09:28:48 The best model epoch 1, val-acc 0.5551
05-25 09:28:48 -----Epoch 24/50-----
05-25 09:28:48 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:49 Train-Acc Source Data 0: 1.0000
05-25 09:28:49 Train-Acc Source Data 1: 1.0000
05-25 09:28:49 Train-Loss Source Classifier: 0.0749
05-25 09:28:49 Train-Loss adv loss: 2.7084
05-25 09:28:49 Train-Loss facto loss: 0.1927
05-25 09:28:49 Train-Loss machine domain loss: 0.0918
05-25 09:28:49 Train-Loss info preserve: 2.0000
05-25 09:28:49 Train-Loss distribution gap: 0.0001
05-25 09:28:49 Val-Acc Target Data: 0.4479
05-25 09:28:49 The best model epoch 1, val-acc 0.5551
05-25 09:28:49 -----Epoch 25/50-----
05-25 09:28:49 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:49 Train-Acc Source Data 0: 1.0000
05-25 09:28:49 Train-Acc Source Data 1: 1.0000
05-25 09:28:49 Train-Loss Source Classifier: 0.0738
05-25 09:28:49 Train-Loss adv loss: 2.7083
05-25 09:28:49 Train-Loss facto loss: 0.1923
05-25 09:28:49 Train-Loss machine domain loss: 0.0905
05-25 09:28:49 Train-Loss info preserve: 2.0000
05-25 09:28:49 Train-Loss distribution gap: 0.0001
05-25 09:28:49 Val-Acc Target Data: 0.4453
05-25 09:28:49 The best model epoch 1, val-acc 0.5551
05-25 09:28:49 -----Epoch 26/50-----
05-25 09:28:49 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:49 Train-Acc Source Data 0: 1.0000
05-25 09:28:49 Train-Acc Source Data 1: 1.0000
05-25 09:28:49 Train-Loss Source Classifier: 0.0726
05-25 09:28:49 Train-Loss adv loss: 2.7083
05-25 09:28:49 Train-Loss facto loss: 0.1924
05-25 09:28:49 Train-Loss machine domain loss: 0.0893
05-25 09:28:49 Train-Loss info preserve: 2.0000
05-25 09:28:49 Train-Loss distribution gap: 0.0001
05-25 09:28:49 Val-Acc Target Data: 0.4427
05-25 09:28:49 The best model epoch 1, val-acc 0.5551
05-25 09:28:49 -----Epoch 27/50-----
05-25 09:28:49 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:50 Train-Acc Source Data 0: 1.0000
05-25 09:28:50 Train-Acc Source Data 1: 1.0000
05-25 09:28:50 Train-Loss Source Classifier: 0.0718
05-25 09:28:50 Train-Loss adv loss: 2.7083
05-25 09:28:50 Train-Loss facto loss: 0.1924
05-25 09:28:50 Train-Loss machine domain loss: 0.0881
05-25 09:28:50 Train-Loss info preserve: 2.0000
05-25 09:28:50 Train-Loss distribution gap: 0.0001
05-25 09:28:50 Val-Acc Target Data: 0.4349
05-25 09:28:50 The best model epoch 1, val-acc 0.5551
05-25 09:28:50 -----Epoch 28/50-----
05-25 09:28:50 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:50 Train-Acc Source Data 0: 1.0000
05-25 09:28:50 Train-Acc Source Data 1: 1.0000
05-25 09:28:50 Train-Loss Source Classifier: 0.0708
05-25 09:28:50 Train-Loss adv loss: 2.7083
05-25 09:28:50 Train-Loss facto loss: 0.1922
05-25 09:28:50 Train-Loss machine domain loss: 0.0869
05-25 09:28:50 Train-Loss info preserve: 2.0000
05-25 09:28:50 Train-Loss distribution gap: 0.0001
05-25 09:28:50 Val-Acc Target Data: 0.4401
05-25 09:28:50 The best model epoch 1, val-acc 0.5551
05-25 09:28:50 -----Epoch 29/50-----
05-25 09:28:50 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:50 Train-Acc Source Data 0: 1.0000
05-25 09:28:50 Train-Acc Source Data 1: 1.0000
05-25 09:28:50 Train-Loss Source Classifier: 0.0694
05-25 09:28:50 Train-Loss adv loss: 2.7082
05-25 09:28:50 Train-Loss facto loss: 0.1922
05-25 09:28:50 Train-Loss machine domain loss: 0.0857
05-25 09:28:50 Train-Loss info preserve: 2.0000
05-25 09:28:50 Train-Loss distribution gap: 0.0001
05-25 09:28:50 Val-Acc Target Data: 0.4375
05-25 09:28:50 The best model epoch 1, val-acc 0.5551
05-25 09:28:50 -----Epoch 30/50-----
05-25 09:28:50 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:50 Train-Acc Source Data 0: 1.0000
05-25 09:28:50 Train-Acc Source Data 1: 1.0000
05-25 09:28:50 Train-Loss Source Classifier: 0.0686
05-25 09:28:50 Train-Loss adv loss: 2.7082
05-25 09:28:50 Train-Loss facto loss: 0.1918
05-25 09:28:50 Train-Loss machine domain loss: 0.0845
05-25 09:28:50 Train-Loss info preserve: 2.0000
05-25 09:28:50 Train-Loss distribution gap: 0.0002
05-25 09:28:50 Val-Acc Target Data: 0.4401
05-25 09:28:50 The best model epoch 1, val-acc 0.5551
05-25 09:28:50 -----Epoch 31/50-----
05-25 09:28:50 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:51 Train-Acc Source Data 0: 1.0000
05-25 09:28:51 Train-Acc Source Data 1: 1.0000
05-25 09:28:51 Train-Loss Source Classifier: 0.0678
05-25 09:28:51 Train-Loss adv loss: 2.7082
05-25 09:28:51 Train-Loss facto loss: 0.1919
05-25 09:28:51 Train-Loss machine domain loss: 0.0833
05-25 09:28:51 Train-Loss info preserve: 2.0000
05-25 09:28:51 Train-Loss distribution gap: 0.0001
05-25 09:28:51 Val-Acc Target Data: 0.4427
05-25 09:28:51 The best model epoch 1, val-acc 0.5551
05-25 09:28:51 -----Epoch 32/50-----
05-25 09:28:51 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:51 Train-Acc Source Data 0: 1.0000
05-25 09:28:51 Train-Acc Source Data 1: 1.0000
05-25 09:28:51 Train-Loss Source Classifier: 0.0667
05-25 09:28:51 Train-Loss adv loss: 2.7082
05-25 09:28:51 Train-Loss facto loss: 0.1917
05-25 09:28:51 Train-Loss machine domain loss: 0.0821
05-25 09:28:51 Train-Loss info preserve: 2.0000
05-25 09:28:51 Train-Loss distribution gap: 0.0001
05-25 09:28:51 Val-Acc Target Data: 0.4401
05-25 09:28:51 The best model epoch 1, val-acc 0.5551
05-25 09:28:51 -----Epoch 33/50-----
05-25 09:28:51 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:51 Train-Acc Source Data 0: 1.0000
05-25 09:28:51 Train-Acc Source Data 1: 1.0000
05-25 09:28:51 Train-Loss Source Classifier: 0.0659
05-25 09:28:51 Train-Loss adv loss: 2.7082
05-25 09:28:51 Train-Loss facto loss: 0.1916
05-25 09:28:51 Train-Loss machine domain loss: 0.0810
05-25 09:28:51 Train-Loss info preserve: 2.0000
05-25 09:28:51 Train-Loss distribution gap: 0.0001
05-25 09:28:51 Val-Acc Target Data: 0.4349
05-25 09:28:51 The best model epoch 1, val-acc 0.5551
05-25 09:28:51 -----Epoch 34/50-----
05-25 09:28:51 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:52 Train-Acc Source Data 0: 1.0000
05-25 09:28:52 Train-Acc Source Data 1: 1.0000
05-25 09:28:52 Train-Loss Source Classifier: 0.0649
05-25 09:28:52 Train-Loss adv loss: 2.7082
05-25 09:28:52 Train-Loss facto loss: 0.1915
05-25 09:28:52 Train-Loss machine domain loss: 0.0799
05-25 09:28:52 Train-Loss info preserve: 2.0000
05-25 09:28:52 Train-Loss distribution gap: 0.0001
05-25 09:28:52 Val-Acc Target Data: 0.4271
05-25 09:28:52 The best model epoch 1, val-acc 0.5551
05-25 09:28:52 -----Epoch 35/50-----
05-25 09:28:52 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
05-25 09:28:52 Train-Acc Source Data 0: 1.0000
05-25 09:28:52 Train-Acc Source Data 1: 1.0000
05-25 09:28:52 Train-Loss Source Classifier: 0.0641
05-25 09:28:52 Train-Loss adv loss: 2.7082
05-25 09:28:52 Train-Loss facto loss: 0.1911
05-25 09:28:52 Train-Loss machine domain loss: 0.0788
05-25 09:28:52 Train-Loss info preserve: 2.0000
05-25 09:28:52 Train-Loss distribution gap: 0.0001
05-25 09:28:52 Val-Acc Target Data: 0.4427
05-25 09:28:52 The best model epoch 1, val-acc 0.5551
05-25 09:28:52 -----Epoch 36/50-----
05-25 09:28:52 current lr: [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
